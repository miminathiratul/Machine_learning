{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll explore a class of algorithms based on decision trees.\n",
    "Decision trees at their root are extremely intuitive.  They\n",
    "encode a series of \"if\" and \"else\" choices, similar to how a person might make a decision.\n",
    "However, which questions to ask, and how to proceed for each answer is entirely learned from the data.\n",
    "\n",
    "For example, if you wanted to create a guide to identifying an animal found in nature, you\n",
    "might ask the following series of questions:\n",
    "\n",
    "- Is the animal bigger or smaller than a meter long?\n",
    "    + *bigger*: does the animal have horns?\n",
    "        - *yes*: are the horns longer than ten centimeters?\n",
    "        - *no*: is the animal wearing a collar\n",
    "    + *smaller*: does the animal have two or four legs?\n",
    "        - *two*: does the animal have wings?\n",
    "        - *four*: does the animal have a bushy tail?\n",
    "\n",
    "and so on.  This binary splitting of questions is the essence of a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main benefit of tree-based models is that they require little preprocessing of the data.\n",
    "They can work with variables of different types (continuous and discrete) and are invariant to scaling of the features.\n",
    "\n",
    "Another benefit is that tree-based models are what is called \"nonparametric\", which means they don't have a fix set of parameters to learn. Instead, a tree model can become more and more flexible, if given more data.\n",
    "In other words, the number of free parameters grows with the number of samples and is not fixed, as for example in linear models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy\n",
    "Shannon's entropy is defined for a system with N possible states as follows:\n",
    "\n",
    "$$\\Large S = -\\sum_{i=1}^{N}p_i \\log_2{p_i},$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $p_i$ is the probability of finding the system in the $i$-th state. This is a very important concept used in physics, information theory, and other areas. Entropy can be described as the degree of chaos in the system. The higher the entropy, the less ordered the system and vice versa. This will help us formalize \"effective data splitting\", which we alluded to in the context of \"20 Questions\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "Let's look at  how entropy can help us to find goods features to build a decision tree, using toy example. Let's predict the color of the ball based on its position.\n",
    "\n",
    "<img align='center' src='image/decision_tree3.png'><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 9 blue balls and 11 yellow balls. If we randomly pull out a ball, then it will be blue with probability $p_1=\\frac{9}{20}$ and yellow with probability $p_2=\\frac{11}{20}$, which gives us an entropy $S_0 = -\\frac{9}{20}\\log_2{\\frac{9}{20}}-\\frac{11}{20}\\log_2{\\frac{11}{20}} \\approx 1$. This value by itself may not tell us much, but let's see how the value changes if we were to break the balls into two groups: with the position less than or equal to 12 and greater than 12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align='center' src='image/balls2.png'><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left group has 13 balls, 8 blue and 5 yellow. The entropy of this group is $S_1 = -\\frac{5}{13}\\log_2{\\frac{5}{13}}-\\frac{8}{13}\\log_2{\\frac{8}{13}} \\approx 0.96$. The right group has 7 balls, 1 blue and 6 yellow. The entropy of the right group is $S_2 = -\\frac{1}{7}\\log_2{\\frac{1}{7}}-\\frac{6}{7}\\log_2{\\frac{6}{7}} \\approx 0.6$. As you can see, entropy has decreased in both groups, more so in the right group. Since entropy is, in fact, the degree of chaos (or uncertainty) in the system, the reduction in entropy is called information gain. Formally, the information gain (IG) for a split based on the variable $Q$ (in this example it's a variable \"$x \\leq 12$\") is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Large IG(Q) = S_O - \\sum_{i=1}^{q}\\frac{N_i}{N}S_i,$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $q$ is the number of groups after the split, $N_i$ is number of objects from the sample in which variable $Q$ is equal to the $i$-th value. In our example, our split yielded two groups ($q = 2$), one with 13 elements ($N_1 = 13$), the other with 7 ($N_2 = 7$). Therefore, we can compute the information gain as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\Large IG(x \\leq 12) = S_0 - \\frac{13}{20}S_1 - \\frac{7}{20}S_2 \\approx 0.16.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By separating the balls into two groups by splitting on \"coordinate is less than or equal to 12\" we have a more ordered system. We can continue to separate them into groups until the balls in each group are all of the same color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align='center' src='image/topic3_credit_scoring_entropy.png'><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the right group, we can easily see that we only need one extra partition using \"coordinate less than or equal to 18\". But, for the left group, we need three more. Note that the entropy of a group where all of the balls are the same color is equal to 0 ($\\log_2{1} = 0$).\n",
    "\n",
    "We have successfully constructed a decision tree that predicts ball color based on its position. This decision tree may not work well if we add any balls because it has perfectly fit to the training set (initial 20 balls). If we wanted to do well in that case, a tree with fewer \"questions\" or splits would be more accurate, even if it does not perfectly fit the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Implement a function for calculating entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n",
    "b = np.array([1, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "def entropy(arr):\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = np.unique(a, return_counts=True)[1] / len(a)\n",
    "-np.sum(probs * np.log2(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entropy(a)) # 0.9612366047228759\n",
    "print(entropy(b)) # 0.5916727785823275"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is a simple binary classification tree that is\n",
    "similar to nearest neighbor classification.  It can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from figures import make_dataset\n",
    "x, y = make_dataset()\n",
    "X = x.reshape(-1, 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Feature X')\n",
    "plt.ylabel('Target y')\n",
    "plt.scatter(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "reg = DecisionTreeRegressor(max_depth=5)\n",
    "reg.fit(X, y)\n",
    "\n",
    "X_fit = np.linspace(-3, 3, 1000).reshape((-1, 1))\n",
    "y_fit_1 = reg.predict(X_fit)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X_fit.ravel(), y_fit_1, color='blue', label=\"prediction\")\n",
    "plt.plot(X.ravel(), y, '.k', label=\"training data\")\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single decision tree allows us to estimate the signal in a non-parametric way,\n",
    "but clearly has some issues.  In some regions, the model shows high bias and\n",
    "under-fits the data.\n",
    "(seen in the long flat lines which don't follow the contours of the data),\n",
    "while in other regions the model shows high variance and over-fits the data\n",
    "(reflected in the narrow spikes which are influenced by noise in single points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classification\n",
    "==================\n",
    "Decision tree classification work very similarly, by assigning all points within a leaf the majority class in that leaf:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from figures import plot_2d_separator\n",
    "\n",
    "\n",
    "X, y = make_blobs(centers=[[0, 0], [1, 1]], random_state=61526, n_samples=100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "plt.figure()\n",
    "plot_2d_separator(clf, X, fill=True)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=np.array(['b', 'r'])[y_train], s=60, alpha=.7, edgecolor='k')\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=np.array(['b', 'r'])[y_test], s=60, edgecolor='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many parameter that control the complexity of a tree, but the one that might be easiest to understand is the maximum depth. This limits how finely the tree can partition the input space, or how many \"if-else\" questions can be asked before deciding which class a sample lies in.\n",
    "\n",
    "This parameter is important to tune for trees and tree-based models. The interactive plot below shows how underfit and overfit looks like for this model. Having a ``max_depth`` of 1 is clearly an underfit model, while a depth of 7 or 8 clearly overfits. The maximum depth a tree can be grown at for this dataset is 8, at which point each leave only contains samples from a single class. This is known as all leaves being \"pure.\"\n",
    "\n",
    "In the interactive plot below, the regions are assigned blue and red colors to indicate the predicted class for that region. The shade of the color indicates the predicted probability for that class (darker = higher probability), while yellow regions indicate an equal predicted probability for either class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from figures import plot_tree_interactive\n",
    "plot_tree_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are fast to train, easy to understand, and often lead to interpretable models. However, single trees often tend to overfit the training data. Playing with the slider above you might notice that the model starts to overfit even before it has a good separation between the classes.\n",
    "\n",
    "Therefore, in practice it is more common to combine multiple trees to produce models that generalize better. The most common methods for combining trees are random forests and gradient boosted trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests are simply many trees, built on different random subsets (drawn with replacement) of the data, and using different random subsets (drawn without replacement) of the features for each split.\n",
    "This makes the trees different from each other, and makes them overfit to different aspects. Then, their predictions are averaged, leading to a smoother estimate that overfits less.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the Optimal Estimator via Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200)\n",
    "parameters = {'max_features':['sqrt', 'log2', 10],\n",
    "              'max_depth':[5, 7, 9]}\n",
    "\n",
    "clf_grid = GridSearchCV(rf, parameters, n_jobs=-1, cv=3)\n",
    "clf_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_grid.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the hyperparameters that are important in building a model:\n",
    "\n",
    "- `n_estimators` — the number of trees in the forest;\n",
    "- `criterion` — the function used to measure the quality of a split;\n",
    "- `max_features` — the number of features to consider when looking for the best split;\n",
    "- `min_samples_leaf` — the minimum number of samples required to be at a leaf node;\n",
    "- `max_depth` — the maximum depth of the tree.\n",
    "\n",
    "#### Practice with random forests using churn data\n",
    " \n",
    "We will use the telecom churn data. This is a classification problem, and we can use AUC for model evaluation.\n",
    "\n",
    "Let's build a simple random forest model with default parameters as a baseline. To make it simple, let's just use numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"Data/telecom_churn.csv\")\n",
    "\n",
    "# Choose the numeric features\n",
    "cols = []\n",
    "for i in df.columns:\n",
    "    if (df[i].dtype == \"float64\") or (df[i].dtype == 'int64'):\n",
    "        cols.append(i)\n",
    "        \n",
    "# Divide the dataset into the input and target\n",
    "X = df[cols].copy() \n",
    "y = np.asarray(df[\"Churn\"], dtype='int8')\n",
    "\n",
    "# Initialize a stratified split of our dataset for the validation process\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize the classifier with the default parameters \n",
    "rfc = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Train it on the training set\n",
    "results = cross_val_score(rfc, X, y, cv=skf, scoring='roc_auc')\n",
    "\n",
    "# Evaluate the accuracy on the test set\n",
    "print(\"CV AUC score: {:.2f}\".format(results.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have AUC score up to 0.84. Let's use gridsearch to find best hyperparameters try to improve this result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the set of parameters for exhaustive search and fit \n",
    "parameters = {'max_features': [4, 7, 10, 13], \n",
    "              'min_samples_leaf': [1, 3, 5, 7],\n",
    "              'max_depth': [5,10,15,20]}\n",
    "rfc = RandomForestClassifier(random_state=42, \n",
    "                             n_jobs=-1)\n",
    "gcv = GridSearchCV(rfc, parameters, n_jobs=-1, cv=5, verbose=1, \n",
    "                   scoring='roc_auc')\n",
    "gcv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcv.best_estimator_, gcv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = gcv.best_estimator_\n",
    "rf.fit(X_train, y_train)\n",
    "rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another option: Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another Ensemble method that can be useful is *Boosting*: here, rather than\n",
    "looking at 200 (say) parallel estimators, We construct a chain of 200 estimators\n",
    "which iteratively refine the results of the previous estimator.\n",
    "The idea is that by sequentially applying very fast, simple models, we can get a\n",
    "total model error which is better than any of the individual pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "clf = GradientBoostingRegressor(n_estimators=100, max_depth=5, \n",
    "                                learning_rate=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Cross-validating Gradient Boosting:\n",
    "Use a grid search to optimize the `learning_rate` and `max_depth` for a Gradient Boosted\n",
    "Decision tree on the digits data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "param_grid = {'learning_rate': [0.01, 0.1, 0.1, 0.5, 1.0],\n",
    "              'max_depth':[1, 3, 5, 7, 9]}\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# split the dataset, apply grid-search\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    random_state=42)\n",
    "skf = StratifiedKFold(5, shuffle=True)\n",
    "model = GradientBoostingClassifier()\n",
    "grid_search = GridSearchCV(model, param_grid, cv=skf,\n",
    "                           scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "results.T.iloc[:,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[results.loc[:, 'rank_test_score'] == 1, :].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid_search.best_estimator_\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "\n",
    "Both RandomForest and GradientBoosting objects expose a `feature_importances_` attribute when fitted. This attribute is one of the most powerful feature of these models. They basically quantify how much each feature contributes to gain in performance in the nodes of the different trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = X_digits[y_digits < 2], y_digits[y_digits < 2]\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=300, n_jobs=1)\n",
    "rf.fit(X, y)\n",
    "print(rf.feature_importances_)  # one value per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical example\n",
    "\n",
    "Let's consider the results of a survey given to visitors of hostels listed on Booking.com and TripAdvisor.com. Our features here are the average ratings for different categories include service quality, room condition, value for money, etc. Our target variable is the hostel's overall rating on the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hostel_data = pd.read_csv(\"Data/hostel_factors.csv\")\n",
    "features = [\"Staff\",\n",
    "\"Hostel booking\",\n",
    "\"Check-in and check-out\",\n",
    "\"Room condition\",\n",
    "\"Shared kitchen condition\",\n",
    "\"Shared space condition\",\n",
    "\"Extra services\",\n",
    "\"General conditions & conveniences\",\n",
    "\"Value for money\",\n",
    "\"Customer Co-creation\"]\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest = RandomForestRegressor(n_estimators=1000, max_features=10,\n",
    "                                random_state=0)\n",
    "\n",
    "forest.fit(hostel_data.drop(['hostel', 'rating'], axis=1), \n",
    "           hostel_data['rating'])\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "feat_importances = sorted(list(zip(features, importances)), \n",
    "                          key=lambda x:x[1], reverse=True)\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "sns.barplot([a[0] for a in feat_importances], \n",
    "            [b[1] for b in feat_importances])\n",
    "\n",
    "plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(importances, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you conclude from the the figure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "- Use the telecom churn data from above.\n",
    "- Use all type of data, not just numerical data.\n",
    "- Split the data into train and test.\n",
    "- Don't use the test data until final evaluation.\n",
    "- Use Logistic regression, random forest and gradient boosting classifier. \n",
    "- Tune the hyperparameters. Evaluate the model with AUC.\n",
    "- Plot the feature importances for random forest and gradient boosting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df = pd.read_csv('Data/telecom_churn.csv')\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = pd.get_dummies(churn_df.drop(columns='Churn')) \n",
    "y = churn_df['Churn'].astype('int')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=42,\n",
    "                                                   stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "print(np.mean(cross_val_score(lr, X_train_scaled,\n",
    "                              y_train, scoring='roc_auc', cv=5)))\n",
    "\n",
    "\n",
    "lr = LogisticRegression(class_weight='balanced')\n",
    "params = {'C':[0.001, 0.01, 0.1, 10, 50, 100]}\n",
    "grid_lr = GridSearchCV(lr, param_grid=params, scoring='roc_auc', cv=5)\n",
    "grid_lr.fit(X_train_scaled, y_train)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(class_weight='balanced')\n",
    "print(np.mean(cross_val_score(rf, X_train, y_train, scoring='roc_auc')))\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "params = {'max_features': [4, 7, 10, 13], \n",
    "          'min_samples_leaf': [1, 3, 5, 7],\n",
    "        'max_depth': [5,10,15,20]}\n",
    "\n",
    "grid_rf = GridSearchCV(rf, param_grid=params, scoring='roc_auc', cv=5)\n",
    "grid_rf.fit(X_train, y_train)\n",
    "print(grid_rf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=100)\n",
    "print(np.mean(cross_val_score(gb, X_train, y_train, \n",
    "                              scoring='roc_auc')))\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=100)\n",
    "params = { 'learning_rate': [0.01, 0.1, 1],\n",
    "        'max_depth': [10,15,20]}\n",
    "\n",
    "grid_gb = GridSearchCV(gb, param_grid=params, scoring='roc_auc', \n",
    "                    cv=5, n_jobs=-1)\n",
    "grid_gb.fit(X_train, y_train)\n",
    "print(grid_gb.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "X = churn_df.drop(columns='Churn').apply(enc.fit_transform)\n",
    "y = churn_df['Churn'].astype('int')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=42,\n",
    "                                                   stratify=y)\n",
    "features = X.columns\n",
    "\n",
    "\n",
    "rf = grid_rf.best_estimator_\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "\n",
    "feat_importances = sorted(list(zip(features, importances)), \n",
    "                          key=lambda x:x[1], reverse=True)\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "sns.barplot([a[0] for a in feat_importances], \n",
    "            [b[1] for b in feat_importances])\n",
    "\n",
    "plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "X = churn_df.drop(columns='Churn').apply(enc.fit_transform)\n",
    "y = churn_df['Churn'].astype('int')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=42,\n",
    "                                                   stratify=y)\n",
    "features = X.columns\n",
    "\n",
    "\n",
    "gb = grid_gb.best_estimator_\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "importances = gb.feature_importances_\n",
    "\n",
    "\n",
    "feat_importances = sorted(list(zip(features, importances)), \n",
    "                          key=lambda x:x[1], reverse=True)\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "sns.barplot([a[0] for a in feat_importances], \n",
    "            [b[1] for b in feat_importances])\n",
    "\n",
    "plt.xticks(rotation=30)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
